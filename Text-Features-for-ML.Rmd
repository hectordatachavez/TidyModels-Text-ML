---
title: 'Building features for predicting classes based on text'
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knit set up
knitr::opts_chunk$set(echo = TRUE)

```

```{r setup_libraries, warning=FALSE, echo=FALSE, message=FALSE}

# Load libraries
library(tidyverse)
library(tidymodels) 
library(tidytext)
library(textrecipes)
library(workflowsets)
library(themis)
library(ggtext)
theme_set(theme_light())

# At the time of making this script (April 2021), the SVM model was only available in the GitHub, development version of the parsnip package. 
#devtools::install_github("tidymodels/parsnip")
#library(parsnip)

```

# Which \#TidyTuesday Netflix titles are movies and which are TV shows?

This projects is inspired by the [Apr 23, 2021 screecast episode](https://www.youtube.com/watch?v=XYj8vyK864Y) of the fabulous [Julia Silge](https://github.com/juliasilge?tab=repositories). In that post, Julia walks through how to build features from text for tidy-modeling. I learned a lot, got inspired, and made this post.

I this GitHub repo I aim to build a Supervised Machine Learning Classification Model that can predict the type of whether a item in the Netflix catalog is either a TV-Show or a Movie based on the description and/or the title.

In doing so, I would like to practice using the `tidymodels` and `textrecipes` R packages by:

-   Building features from description text

-   Prepare different model recipes (`workflowsets`) based on these features from text

    -   tokens from description
    -   tokens from description + tokens from title
    -   n-grams from description
    -   tokens from description + sub-word features from the title

-   Set up a SVM classification model to predict type (TV-Show or Movie) that trains, test, and compare all recipes

-   Compare models performance after 10-fold cross-validation

-   Identify and select the best model

-   Plot feature importance

The data used for this project is the [week 17 dataset on Netflix Titles](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-20/readme.md) of the [\#TidyTuesday](https://github.com/rfordatascience/tidytuesday).

## Load Data

```{r load_data, echo=FALSE, warning=FALSE}

# Load data from netflix movie and TV show titles from the tidyverse
netflix_titles <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv")

```

## Explore Data

There are more Movies than TV-shows in the dataset.

```{r explore_data, echo=FALSE, warning=FALSE}

# How many titles in the dataset?
netflix_titles %>%
  count(type) 

# What do the descriptions look like? 
netflix_titles %>%
  slice_sample(n = 10) %>%
  pull(description)

# What do titles look like?
netflix_titles %>%
  slice_sample(n = 10) %>%
  pull(title)

```

### Description field

This column contains the description of the Movies and TV Shows: are all around 150 characters long.

A histogram of the number of characters per description by type also highlights the issue of balance in the sample: there are more movies than TV Shows.

```{r data_exploration_plot, echo=FALSE, warning=FALSE}

# Histogram of number of characters per description by type
netflix_titles %>%
  select(type, description) %>%
  mutate(n_char = nchar(description)) %>%
  ggplot(aes(x=n_char, fill=type)) +
  geom_histogram(show.legend = FALSE, alpha = 0.8) + 
  labs(
    x = "Number of Characters", y = "Frequency",
    title = "<strong>Number of characters Netflix descriptions of <span style = 'color:#F8766D'>Movies</span> and <span style = 'color:#00BFC4'>TV Shows</span> by frequency</strong>",
    subtitle = NULL
  ) +
  theme(plot.title = element_markdown(),  
        plot.title.position = "plot")

```

Word frequency analysis of the description field show some differences in the top words for each category as well as some similarities.

```{r data_exploration_description_plot, echo=FALSE, warning=FALSE}

# What are the top words in the description each category (Movie, TV Show)?
netflix_titles %>%
  # create tokens at the word level for the description column
  unnest_tokens(word, description) %>%
  # remove stop-words
  anti_join(get_stopwords()) %>%
  # get frequencies for each word for each category, sort in desc order
  count(type, word, sort = TRUE) %>%
  # group by category
  group_by(type) %>%
  # get the top 15 most commonly used words
  slice_max(n, n = 15) %>%
  # ungroup
  ungroup() %>%
  # reorder word column by frequency and category
  mutate(word = reorder_within(word, n, type)) %>%
  # Plot frequencies
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  scale_y_reordered() +
  facet_wrap(~type, scales = "free") +
  labs(
    x = "Word frequency", y = NULL,
    title = "<strong>Top words in Netflix descriptions of <span style = 'color:#F8766D'>Movies</span> and <span style = 'color:#00BFC4'>TV Shows</span> by frequency</strong>",
    subtitle = "After removing stop words"
  ) +
  theme(plot.title = element_markdown(),  
        plot.title.position = "plot", 
        strip.background = element_blank(),
        strip.text.x = element_blank() )
```

#### Bi-grams in the description field

We can also tokenize text into consecutive sequences of words, called `n-grams`. This way, we can use how often word X is followed by word Y in model building.

```{r data_exploration_description_bigram_plot}
# What are the top words in the description each category (Movie, TV Show)?
netflix_titles %>%
  # create tokens bigram tokens for the description field
  unnest_tokens(bigram, description, token = "ngrams", n = 2 ) %>%
  # separaye the bi-gram into two columns: word1 and word 2
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  # remove stop words from the bi grams
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  # Unite back the filteres bi-grams into one column
  unite(bigram, word1, word2, sep = " ") %>%
  # get frequencies for each word for each category, sort in desc order
  count(type, bigram, sort = TRUE) %>%
  # group by category
  group_by(type) %>%
  # get the top 15 most commonly used words
  slice_max(n, n = 15) %>%
  # ungroup
  ungroup() %>%
  # reorder word column by frequency and category
  mutate(bigram = reorder_within(bigram, n, type)) %>%
  # Plot frequencies
  ggplot(aes(n, bigram, fill = type)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  scale_y_reordered() +
  facet_wrap(~type, scales = "free") +
  labs(
    x = "Word frequency", y = NULL,
    title = "<strong>Top bigrams in Netflix descriptions of <span style = 'color:#F8766D'>Movies</span> and <span style = 'color:#00BFC4'>TV Shows</span> by frequency</strong>",
    subtitle = "After removing stop words"
  ) +
  theme(plot.title = element_markdown(),  
        plot.title.position = "plot",
        strip.background = element_blank(),
        strip.text.x = element_blank())

```



### Title field

Words in the title of the TV Show or Movie are not unique to each category. "Love", for example, is one of the frequent word in the titles of both movies and TV shows. Similarly with "world", "life", "little", "one". This field does not seem to contain enough information to help a model differentiate TV Shows from Movies based on the title.

```{r data_exploration_title_plot, echo=FALSE, warning=FALSE}

# What are the top words in the title of each category (Movie, TV Show)?
netflix_titles %>%
  # create tokens at the word level for the title column
  unnest_tokens(word, title) %>%
  # remove stopwords
  anti_join(get_stopwords()) %>%
  # get frequencies for each word for each category, sort in desc order
  count(type, word, sort = TRUE) %>%
  # group by category
  group_by(type) %>%
  # get the top 15 most commonly used words
  slice_max(n, n = 15) %>%
  # ungroup
  ungroup() %>%
  # reorder word column by frequency and category
  mutate(word = reorder_within(word, n, type)) %>%
  # Plot frequencies
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  scale_y_reordered() +
  facet_wrap(~type, scales = "free") +
  labs(
    x = "Word frequency", y = NULL,
    title = "<strong>Top words in Titles of Netflix <span style = 'color:#F8766D'>Movies</span> and <span style = 'color:#00BFC4'>TV Shows</span> by frequency</strong>",
    subtitle = "After removing stop words"
  ) +
  theme(plot.title = element_markdown(),  
        plot.title.position = "plot", 
        strip.background = element_blank(),
        strip.text.x = element_blank())
```

#### Bi-grams in the title field

This feature does not contain much information to differentiate between Netflix Movies and TV-shows. It will not be used in models recipes.

```{r data_exploration_tile_bigram_plot}
# What are the top words in the description each category (Movie, TV Show)?
netflix_titles %>%
  # create tokens bigram tokens for the description field
  unnest_tokens(bigram, title, token = "ngrams", n = 2 ) %>%
  # remove NAs
  drop_na(bigram) %>%
  # separate the bi-gram into two columns: word1 and word 2
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  # remove stop words from the bi grams
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  # Unite back the filteres bi-grams into one column
  unite(bigram, word1, word2, sep = " ") %>%
  # get frequencies for each word for each category, sort in desc order
  count(type, bigram, sort = TRUE) %>%
  # group by category
  group_by(type) %>%
  # get the top 15 most commonly used words
  slice_max(n, n = 15) %>%
  # ungroup
  ungroup() %>%
  # reorder word column by frequency and category
  mutate(bigram = reorder_within(bigram, n, type)) %>%
  # Plot frequencies
  ggplot(aes(n, bigram, fill = type)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  scale_y_reordered() +
  facet_wrap(~type, scales = "free") +
  labs(
    x = "Word frequency", y = NULL,
    title = "<strong>Top bigrams in Titles of Netflix <span style = 'color:#F8766D'>Movies</span> and <span style = 'color:#00BFC4'>TV Shows</span> by frequency</strong>",
    subtitle = "After removing stop words"
  ) +
  theme(plot.title = element_markdown(),  
        plot.title.position = "plot",
        strip.background = element_blank(),
        strip.text.x = element_blank())

```


## Set up for tidy-modeling

We first define the split in the original data into training and testing set (75%, 25%, rexpectively). Then, we define the cross-validation folds in the training set.

```{r model_prep, echo=FALSE, warning=FALSE}

# Set seed for reproducibility
set.seed(123)
# Define the splits in the data
netflix_split <- netflix_titles %>%
  select(type, description, title) %>%
  # stratify split samples by type (TV Shows vs Movie). The sample is not balanced (not a 50:50 split). Thus, we must take care that the splitting is balanced.
  # The inital split is 3/4 to training, 1/4 to testing
  initial_split(strata = type)

# Define the training set
netflix_train <- training(netflix_split)
# Define the testing set
netflix_test <- testing(netflix_split)


# Set seed for reproducibility
set.seed(234)
# Define the CV folds in the training dataset, stratified again by type
netflix_folds <- vfold_cv(netflix_train, strata = type)

# Print the sample division by fold
netflix_folds
```


## Define model recipes

We will define four model recipes:
    -   using only tokens from description
    -   using tokens from description + tokens from title
    -   using n-grams from description
    -   using tokens from description + sub-word features from the title

For all recipes, we will use `step_smote` on type. This function generates new examples of the minority class using nearest neighbors of these cases. It helps us create new data as the dataset is unbalanced: move movies than TV shows.


```{r model_recipes, echo=FALSE, warning=FALSE}
# create our feature engineering recipe and our model, and then put them together in a modeling workflow

# Recipe 1: using only tokens from description
netflix_rec1 <- 
  # Define the model and the (training) data
  recipe(type ~ description, data = netflix_train) %>%
  # Define the steps of processing
  # tokenize text in description
  step_tokenize(description) %>%
  # filter tokenlist based on frequency
  step_tokenfilter(description, max_tokens = 1e3) %>%
  # convert tokelist into a term-frequency-inverse document freq (columns with 0-1)
  step_tfidf(description) %>%
  # normalize all 0-1 columns
  step_normalize(all_numeric_predictors()) %>%
  # generate new examples of the minority class using nearest neighbors of these cases
  step_smote(type)

# Print the recipe
netflix_rec1



# Recipe 2: using only tokens from description + tokens from title
netflix_rec2 <- 
  # Define the model and the (training) data
  recipe(type ~ description + title, data = netflix_train) %>%
  # Define the steps of processing
  # tokenize text
  step_tokenize(description, title) %>%
  # filter tokenlist based on frequency
  step_tokenfilter(description, title, max_tokens = 1e3) %>%
  # convert tokelist into a term-frequency-inverse document freq (columns with 0-1)
  step_tfidf(description, title) %>%
  # normalize all 0-1 columns
  step_normalize(all_numeric_predictors()) %>%
  # generate new examples of the minority class using nearest neighbors of these cases
  step_smote(type)

# Print the recipe
netflix_rec2


# Recipe 3: using n-grams from description
netflix_rec3 <- 
  # Define the model and the (training) data
  recipe(type ~ description, data = netflix_train) %>%
  # Define the steps of processing
  # tokenize text
  step_tokenize(description) %>%
  # create n-grams: include mono and bi-grams
  step_ngram(description, min_num_tokens = 1, num_tokens = 2) %>%
  # filter tokenlist based on frequency
  step_tokenfilter(description, max_tokens = 1e3) %>%
  # convert tokelist into a term-frequency-inverse (columns with 0-1)
  step_tfidf(description) %>%
  # normalize all 0-1 columns
  step_normalize(all_numeric_predictors()) %>%
  # generate new examples of the minority class using nearest neighbors of these cases
  step_smote(type)

# Print the recipe
netflix_rec3




netflix_rec4 <- 
  # Define the model and the (training) data
  # Type is explained by the text in description and in title
  recipe(type ~ description + title, data = netflix_train) %>%
  # Define the steps of processing
  # tokenize text in description
  step_tokenize(description) %>%
  # filter tokenlist based on frequency
  step_tokenfilter(description, max_tokens = 1e3) %>%
  # convert tokelist into a term-frequency-inverse document freq (columns with 0-1)
  step_tfidf(description) %>%
  # tokenize
  step_tokenize(title,
    engine = "tokenizers.bpe",
    training_options = list(vocab_size = 200)
  ) %>%
  # filter tokenlist based on frequency
  step_tokenfilter(title, max_tokens = 200) %>%
  # convert tokelist into a term-frequency (columns with 0-1)
  step_tfidf(title) %>%
  # normalize all 0-1 columns
  step_normalize(all_numeric_predictors()) %>%
  # step_smote generates new examples of the minority class using nearest neighbors of these cases
  step_smote(type)


# Print the recipe
netflix_rec4


```

## Define model: tye and engine

For this project, we will use one model: [a linear support-vector machine classification model](https://parsnip.tidymodels.org/reference/svm_linear.html). All recipes defines above will be passed to this model.

At the time of making this script (April 2021), the SVM model was only available in the GitHub, development version of the parsnip package.

```{r define_model_svm}
# Define the model used to predict type by description tokens
# SVM linear
svm_spec <- svm_linear() %>%
  # set engine to linear SVM
  set_engine("LiblineaR") %>%
  # define more to classification (categorical prediction)
  set_mode("classification")
```


## Define worlflow set

    -   using only tokens from description
    -   using tokens from description + tokens from title
    -   using n-grams from description
    -   using tokens from description + sub-word features from the title

```{r workflow_set}

# Define workflow_set
netflix_wf <- workflow_set(
  # define the pre-processing recipes
                preproc =list(base = netflix_rec1,
                              tokens_desc_title = netflix_rec2,
                              ngrams_desc = netflix_rec3,
                              tokens_desc_subwork_title = netflix_rec4) ,
  # define the models: in this case only 1
                models = list(svm_spec)
)

# Print workflow
netflix_wf


```

## Fit model
```{r fit_svm2workflow}
# Set up parallel processing
doParallel::registerDoParallel()

# set seed for reproducibility
set.seed(123)

# Fit the workflow to the CV folds of the training data
svm_rs <- fit_resamples(
  # Pass the workflow
  netflix_wf,
  # to the folds
  netflix_folds,
  # linear SVM model does not support class probabilities, so we need to set a custom metric_set() that only includes metrics for hard class probabilities
  metrics = metric_set(accuracy, recall, precision),
  # save the preductions from the model to make a confusion matrix
  control = control_resamples(save_pred = TRUE)
)
```


## Model Performance

```{r model_performance}
# Get the metrics from the model
collect_metrics(svm_rs)

# We can use conf_mat_resampled() to compute a separate confusion matrix for each resample, and then average the cell counts
svm_rs %>%
  conf_mat_resampled(tidy = FALSE) %>%
  autoplot()

```

## Final Model Fit

```{r final_model_fit}
# Fit our model on last time to the whole training set (rather than resampled data) and evaluate on the testing set.

# The last fit function does the model training and the testing automatically, following the workflow defined
final_fitted <- last_fit(
  netflix_wf_comb,
  # contains the training and the testing datasets
  netflix_split_comb,
  # define custom metrics
  metrics = metric_set(accuracy, recall, precision)
)

```

```{r final_model_performance}
# get the performance metrics of the fitted model--about the same as in the CV fit
collect_metrics(final_fitted)

# Make a confusion matrix
collect_predictions(final_fitted) %>%
  conf_mat(type, .pred_class)
```

## Feature Importance

```{r pull_workflow}
# Extract the workflow object from the final model object
netflix_fit <- pull_workflow_fit(final_fitted$.workflow[[1]])

# passing he tidy function to the pulled workflow object arranges the estimates as a tibble
tidy(netflix_fit) %>%
  # arrange based on the coefficient of each term
  arrange(estimate)
```

```{r feature_importance_plot}
# get the tibble with coeficients and terms
tidy(netflix_fit) %>%
  # remove the estimate for 'Bias' the intercept of this model
  filter(term != "Bias") %>%
  # group by whether or not the estimate is >0, rename this to "sign"
  group_by(sign = estimate > 0) %>%
  # get only the top 15 highest estimate (in absolute terms)
  slice_max(abs(estimate), n = 15) %>%
  # ungroup
  ungroup() %>%
  # Clean the terms and the sign column
  mutate(
    term = str_remove(term, "tfidf_description_"),
    term = str_remove(term, "tf_title_"),
    sign = if_else(sign, "More from TV shows", "More from movies")
  ) %>%
  # set up the plot
  ggplot(aes(abs(estimate), fct_reorder(term, abs(estimate)), fill = sign)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~sign, scales = "free") +
  labs(
    x = "Coefficient from linear SVM", y = NULL,
    title = "<strong>Which words are most predictive of <span style = 'color:#F8766D'>Movies</span> vs. <span style = 'color:#00BFC4'>TV Shows</span>?</strong>",
    subtitle = "For description text of movies and TV shows on Netflix"
  ) +
  theme(plot.title = element_markdown(),  
        plot.title.position = "plot" )
```
